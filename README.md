# ADS Programmer Coding Assessment

Jing Li's Github Submission for ADS Programmer Coding Assessment

---

## Repository Structure

```
ads_coding_assess/
├── question_1_sdtm/     
├── question_2_adam/      
├── question_3_tlg/        
├── question_4_agent/      
└── README.md
```

---

## Question 1: SDTM DS Domain Creation

**Objective:** Create an SDTM DS (Disposition) domain from raw clinical data using the `sdtm.oak` package.

### Files Included

| File | Description |
|------|-------------|
| `01_create_ds_domain.R` | R script that creates the DS domain using sdtm.oak functions including `assign_ct()`, `assign_no_ct()`, `assign_datetime()`, `derive_seq()`, and `derive_study_day()` |
| `01_create_ds_domain.log` | Execution log generated by `logrx::axecute()` documenting the complete run with no errors |
| `sdtm_ds_output.csv` | Output DS domain dataset in CSV format |

### Note on Controlled Terminology

Some records in the DS domain have missing VISIT and VISITNUM values due to the controlled terminology file (`sdtm_ct.csv`) provided on GitHub being incomplete. The CT file did not contain mappings for certain visit terms (e.g., "Unscheduled" visits), resulting in unmapped values for those records.

---

## Question 2: ADaM ADSL Dataset Derivation

**Objective:** Create an ADaM ADSL (Subject-Level Analysis Dataset) using the `admiral` package with specific derived variables.

### Files Included

| File | Description |
|------|-------------|
| `create_adsl.R` | R script that derives ADSL variables using admiral functions including `derive_vars_dtm()`, `derive_vars_merged()`, and `derive_vars_extreme_event()` |
| `create_adsl.log` | Execution log generated by `logrx::axecute()` confirming successful execution with no errors |
| `adsl_output.csv` | Output ADSL dataset in CSV format |

### Derived Variables

- **AGEGR9 / AGEGR9N**: Age group categories (<18, 18-50, >50)
- **TRTSDTM / TRTSTMF**: Treatment start datetime with time imputation flag
- **ITTFL**: Intent-to-treat population flag
- **LSTAVLDT**: Last available date (maximum of VS, AE, DS dates and TRTEDT)

---

## Question 3: Adverse Events Summary & Visualizations

**Objective:** Create summary tables and visualizations for adverse events analysis using `gtsummary` and `ggplot2`.

### Files Included

| File | Description |
|------|-------------|
| `01_create_ae_summary_table.R` | R script that creates the AESOC summary table using gtsummary |
| `01_create_ae_summary_table.log` | Execution log with no errors recorded |
| `02_create_visualizations.R` | R script that creates the severity distribution bar chart and top 10 AE forest plot |
| `02_create_visualizations.log` | Execution log with no errors recorded |
| `output_files/ae_summary_table.pdf` | PDF output of the AESOC summary table |
| `output_files/ae_severity_distribution.png` | Stacked bar chart showing AE severity distribution by treatment arm |
| `output_files/top10_frequent_ae.png` | Forest plot showing top 10 most frequent AEs with 95% confidence intervals |

### Outputs

1. **AESOC Summary Table**: Treatment-emergent AEs summarized by System Organ Class using `gtsummary::tbl_summary()`, stratified by treatment arm (Placebo, Xanomeline High Dose, Xanomeline Low Dose) with n (%) statistics

2. **Severity Distribution Plot**: MILD/MODERATE/SEVERE distribution across treatment arms

3. **Top 10 AE Incidence Plot**: Top 10 most frequent adverse events with incidence rates and 95% Clopper-Pearson exact confidence intervals

---

## Question 4: AI Clinical Trial Data Agent

**Objective:** Develop a generative AI assistant that translates natural language questions into structured pandas queries for adverse events data.

### Files Included

| File | Description |
|------|-------------|
| `01_create_agent.py` | Main Python module containing the `ClinicalTrialDataAgent` class with OpenAI LLM integration |
| `test_agent.py` | Test script that runs 3 example queries to validate the agent functionality |

### How It Works

1. **Schema Definition**: The agent uses a detailed schema (`AE_SCHEMA`) that describes each column in the AE dataset, including labels, descriptions, valid values, and semantic mappings

2. **Question Parsing**: Natural language questions are sent to GPT-4o-mini along with the schema context, and the LLM returns structured JSON specifying which column to query and what value to filter for

3. **Query Execution**: The parsed query is executed against the pandas DataFrame using either exact or contains matching (case-insensitive)

4. **Results**: Returns unique subject count, subject IDs, total records, and sample data

### Example Queries

```
"How many patients had severe adverse events?"     → AESEV = "SEVERE"
"Show me subjects with cardiac disorders"          → AESOC contains "CARDIAC"
"Which patients experienced headache?"             → AETERM contains "HEADACHE"
```

### Running the Test

```bash
cd question_4_agent
python -m venv test_env
source test_env/bin/activate
pip install pandas openai
python test_agent.py
```
---

## Requirements

### R Packages
- admiral
- sdtm.oak
- pharmaverseraw
- pharmaverseadam
- pharmaversesdtm
- dplyr
- gtsummary
- ggplot2
- binom
- logrx

### Python Packages
- pandas
- openai

---

## Notes

- All R scripts were executed using `logrx::axecute()` to generate comprehensive execution logs
- All log files show successful execution with no errors
- **CSV and PDF output formats** were used for user convenience, allowing files to be viewed directly on GitHub without needing to download them
- The Python virtual environment (`test_env/`) and API key files are excluded from version control

