# ADS Programmer Coding Assessment

Jing Li's Github Submission for ADS Programmer Coding Assessment

---

## Requirement Packages

### R Packages
- admiral
- sdtm.oak
- pharmaverseraw
- pharmaverseadam
- pharmaversesdtm
- dplyr
- gtsummary
- ggplot2
- binom
- logrx

### Python Packages
- pandas
- openai


## Repository Structure

```
ads_coding_assess/
├── question_1_sdtm/     
├── question_2_adam/      
├── question_3_tlg/
    ├── output_files/       
├── question_4_agent/
└── README.md
```

---

## Question 1: SDTM DS Domain Creation

**Objective:** Create an SDTM DS domain from raw clinical data using the `sdtm.oak` package.

### Files Included

| File | Description |
|------|-------------|
| `01_create_ds_domain.R` | R script that creates the DS domain using sdtm.oak functions including `assign_ct()`, `assign_no_ct()`, `assign_datetime()`, `derive_seq()`, and `derive_study_day()` |
| `01_create_ds_domain.log` | Execution log generated by `logrx::axecute()` |
| `sdtm_ds_output.csv` | Output DS domain dataset in CSV format |

### Note on Controlled Terminology

Some records in the DS domain have missing/invalid VISIT and VISITNUM values due to the controlled terminology file (`sdtm_ct.csv`) provided on GitHub being incomplete. The CT file did not contain mappings for certain visit terms (e.g., "Unscheduled" visits), resulting in unmapped values for those records.

---

## Question 2: ADaM ADSL Dataset Creation

**Objective:** Create ADaM ADSL using the `admiral` package with specific derived variables.

### Files Included

| File | Description |
|------|-------------|
| `create_adsl.R` | R script that derives ADSL variables using admiral functions including `derive_vars_dtm()`, `derive_vars_merged()`, and `derive_vars_extreme_event()` |
| `create_adsl.log` | Execution log generated by `logrx::axecute()` |
| `adsl_output.csv` | Output ADSL dataset in CSV format |

### Derived Variables

- **AGEGR9 / AGEGR9N**: Age group categories (<18 (1), 18-50 (2), >50 (3))
- **TRTSDTM / TRTSTMF**: Treatment start datetime with time imputation flag
- **ITTFL**: Intent to treat flag
- **LSTAVLDT**: Last available date (max of VS, AE, DS dates and TRTEDT)

---

## Question 3: Adverse Events Reporting

**Objective:** Create summary tables and visualizations for adverse events analysis using `gtsummary` and `ggplot2`.

### Files Included

| File | Description |
|------|-------------|
| `01_create_ae_summary_table.R` | R script that creates the AESOC summary table using gtsummary |
| `01_create_ae_summary_table.log` | Execution log with no errors recorded |
| `02_create_visualizations.R` | R script that creates the severity distribution bar chart and top 10 AE forest plot |
| `02_create_visualizations.log` | Execution log with no errors recorded |
| `output_files/ae_summary_table.pdf` | PDF output of the AESOC summary table |
| `output_files/ae_severity_distribution.png` | Stacked bar chart showing AE severity distribution by treatment arm |
| `output_files/top10_frequent_ae.png` | CI plot showing top 10 most frequent AEs with 95% confidence intervals |

### Outputs

1. **AESOC Summary Table**: Treatment emergent AEs summarized by System Organ Class using `gtsummary::tbl_summary()`, stratified by treatment arm (Placebo, Xanomeline High Dose, Xanomeline Low Dose) with n (%) statistics

2. **AE Severity Distribution Plot**: MILD/MODERATE/SEVERE distribution across treatment arms

3. **Top 10 Most Frequent AEs Plot**: Top 10 most frequent adverse events with incidence rates and 95% Clopper-Pearson exact confidence intervals

---

## Question 4: GenAI Clinical Data Assistant

**Objective:** Develop a generative AI assistant that takes natural language questions into structured pandas queries for AE data.

### Files Included

| File | Description |
|------|-------------|
| `01_create_agent.py` | Python script containing the `ClinicalTrialDataAgent` class with OpenAI LLM integration |
| `test_agent.py` | Test script that runs 3 example queries |

### How It Works

1. **Schema Definition**: The agent takes in a schema that describes each column in the AE dataset, including labels, descriptions, and valid values

2. **Question Parsing**: Questions are sent to GPT-4o-mini model along with the schema, and the LLM returns a structured JSON specifying which column to query and what value to filter for

3. **Query Execution**: The parsed query is executed against the pandas DataFrame using either exact or contains matching 

4. **Results**: Returns unique subject count, subject IDs, total records, and sample data

### Example Queries

```
"How many patients had severe adverse events?" This will map as AESEV = "SEVERE"
"Show me subjects with cardiac disorders" This will map as AESOC contains "CARDIAC"
"Which patients experienced headache?" This will map as AETERM contains "HEADACHE"
```

### Running the Test

```bash
cd question_4_agent
python -m venv test_env
source test_env/bin/activate
pip install pandas openai
python test_agent.py
```

---

## Notes

- All R scripts were executed using `logrx::axecute()` to generate comprehensive execution logs
- All log files show successful execution with no errors
- **CSV and PDF output formats** were used for user convenience, allowing files to be viewed directly on GitHub without needing to download them
- The Python virtual environment (`test_env/`) and API key files are excluded

